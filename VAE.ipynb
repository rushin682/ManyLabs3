{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists, isdir, isfile, join\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
    "\n",
    "from dataloader import ML3Dataset, get_category_level_data\n",
    "from autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch : 1.1.0\n",
      "numpy : 1.16.1\n",
      "pandas : 0.24.1\n"
     ]
    }
   ],
   "source": [
    "print(\"torch : {}\".format(torch.__version__))\n",
    "print(\"numpy : {}\".format(np.__version__))\n",
    "print(\"pandas : {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_dir DIR] [--category CSV_File]\n",
      "                             [--workers WORKERS] [--epochs EPOCHS]\n",
      "                             [-b BATCH_SIZE] [--lr LR]\n",
      "                             [--weight-decay WEIGHT_DECAY] [--log-interval N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/bravos/Library/Jupyter/runtime/kernel-766551f1-a6b8-4744-993d-af2acafef3cd.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bravos/Documents/CourseWork/Semester2/for_Sem2/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Hyperparameters')\n",
    "parser.add_argument('--data_dir', default='files', metavar='DIR', help='path to csv')\n",
    "parser.add_argument('--category', default='intrinsic', metavar='CSV_File', help='The csv category')\n",
    "parser.add_argument('--workers', default=1, type=int)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('-b', '--batch-size', default=8, type=int, help='mini-batch size') #could use batch_size 12\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float, help='initial learning rate')\n",
    "parser.add_argument('--weight-decay', default=0.1, type=float, help='weight decay')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "best_val_loss = 0\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "data_dir = '/Users/bravos/Documents/CourseWork/Semester2/cs536-ML/ManyLab3/'\n",
    "files_dir = join(data_dir, args.data_dir)\n",
    "#all_data_dir = join(data_dir, 'ML3')\n",
    "\n",
    "assert isdir(data_dir) and isdir(files_dir)\n",
    "\n",
    "study_data = get_category_level_data(files_dir, category_type=args.category)\n",
    "print(len(study_data['train']), len(study_data['valid']))\n",
    "\n",
    "kwargs = {}\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    ML3Dataset(study_data['train'], transform=None),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "valid_loader = data.DataLoader(\n",
    "    ML3Dataset(study_data['valid'], transform=None),\n",
    "    batch_size=args.batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_shape, 8)\n",
    "        self.fc21 = nn.Linear(8, self.n_latent)\n",
    "        self.fc22 = nn.Linear(8, self.n_latent)\n",
    "        self.fc3 = nn.Linear(self.n_latent, 8)\n",
    "        self.fc4 = nn.Linear(8, self.input_shape)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_shape))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_sizes = {x: len(study_datasets[x]) for x in ['train', 'valid']}          \n",
    "\n",
    "model = VAE(input_shape=15, n_latent=2).to(device)\n",
    "\n",
    "# We use an initial learning rate of 0.0001 that is decayed by a factor of\n",
    "# 10 each time the validation loss plateaus after an epoch, and pick the\n",
    "# model with the lowest validation loss\n",
    "optimizer = optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=10, verbose=True)\n",
    "    \n",
    "    \n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (row, _) in enumerate(train_loader):\n",
    "        row = row.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(row)\n",
    "        loss = loss_function(recon_batch, row, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(row), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(row)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def valid(epoch):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (row, _) in enumerate(valid_loader):\n",
    "            row = row.to(device)\n",
    "            recon_batch, mu, logvar = model(row)\n",
    "            valid_loss += loss_function(recon_batch, row, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(row.size(0), 8)\n",
    "                comparison = torch.cat([row[:n],\n",
    "                                      recon_batch[:n]])\n",
    "                print (comparison)\n",
    "#                 See what happens when you try to concatenate two torch rows. It should probably we one row elongated.\n",
    "#                 But we need two rows to compare each index or two columns as such for the same.\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    print('====> Valid set loss: {:.4f}'.format(valid_loss))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        valid(epoch)\n",
    "#         with torch.no_grad():\n",
    "#             sample = torch.randn(64, 20).to(device)\n",
    "#             sample = model.decode(sample).cpu()\n",
    "#             save_image(sample.view(64, 1, 28, 28),\n",
    "#                        'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
